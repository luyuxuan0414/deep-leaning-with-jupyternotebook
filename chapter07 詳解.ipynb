{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
    "\n",
    "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
    "\n",
    "This notebook was generated for TensorFlow 2.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "# Working with Keras: A deep dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## A spectrum of workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Different ways to build Keras models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The Sequential model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**The `Sequential` class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#構建網絡\n",
    "#使用sequential類\n",
    "#“順序模型適用於簡單的層堆棧，其中每一層只有一個輸入張量和一個輸出張量\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "    #最後一層->對於每個輸入樣本，網絡都會輸出一個10維向量\n",
    "    #softmax激活後的輸出可以理解成該概率分佈，概率總和爲1\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Incrementally building a Sequential model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#input_dim=28*28 表示輸入的是一個28*28=784長度的vector\n",
    "#表示image units=500 表示該層的hidden layer 要有500個neuron\n",
    "model = keras.Sequential()\n",
    "#model.add增加一個dense全連接層\n",
    "model.add(layers.Dense(64, activation=\"relu\"))#第一層 units = 64 代表有64個神經元\n",
    "model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "#最後一層即為輸出層 units =10 代表有10個神經元，要訓練10個輸出0~1的二元分類器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Calling a model for the first time to build it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_6/kernel:0' shape=(3, 64) dtype=float32, numpy=\n",
       " array([[-0.19585726, -0.1938601 ,  0.2968989 ,  0.06495699, -0.1261902 ,\n",
       "         -0.14504796, -0.03266022, -0.09787755, -0.0653235 ,  0.16818866,\n",
       "         -0.04639488,  0.21391666,  0.21613103, -0.13551931, -0.16494498,\n",
       "          0.25334668,  0.15784869, -0.02862865,  0.15919366,  0.11915848,\n",
       "         -0.21949488,  0.11491081,  0.1993717 , -0.00796902, -0.09430268,\n",
       "          0.00289622,  0.02638593, -0.21994822, -0.2638069 ,  0.09703085,\n",
       "         -0.00732517, -0.21669999,  0.15402141,  0.00655469,  0.05564815,\n",
       "          0.03990284, -0.09421071,  0.1842897 , -0.0347566 , -0.01654267,\n",
       "         -0.06246196,  0.13632959, -0.15034537, -0.28302127,  0.1871351 ,\n",
       "          0.20969051, -0.03564873,  0.18708602, -0.08652431,  0.26326257,\n",
       "         -0.2003632 , -0.0536883 , -0.17979011, -0.13156416,  0.19536224,\n",
       "          0.02830544,  0.04423383, -0.24093848, -0.01009667, -0.2629757 ,\n",
       "         -0.13964576,  0.20191193, -0.06729034, -0.0530162 ],\n",
       "        [ 0.08811316,  0.16518864,  0.0867016 ,  0.24605739,  0.03674594,\n",
       "          0.15341774, -0.12897718, -0.12577282, -0.23980948, -0.21482347,\n",
       "         -0.01534912,  0.27754438, -0.1013869 , -0.16460887, -0.13528985,\n",
       "         -0.220523  , -0.1502534 ,  0.12726718, -0.26654986,  0.03708783,\n",
       "          0.09339258,  0.13488182, -0.11336599, -0.18988383,  0.09041503,\n",
       "         -0.01069713, -0.11798218,  0.06969723,  0.01252568, -0.29315338,\n",
       "          0.10276249, -0.23355645, -0.21102543, -0.03488731, -0.27742755,\n",
       "          0.07748058,  0.24613363,  0.03956121, -0.22982912,  0.281883  ,\n",
       "         -0.24240118, -0.28339633,  0.1551888 ,  0.08649713, -0.14624675,\n",
       "          0.24189496, -0.14956069, -0.11778505,  0.01575831,  0.06632128,\n",
       "         -0.14588444, -0.25372386,  0.19875851,  0.01539028, -0.00316462,\n",
       "         -0.24892141,  0.11188275, -0.01190311, -0.27623135, -0.21372259,\n",
       "          0.14694211,  0.26241577,  0.10467708,  0.1303994 ],\n",
       "        [ 0.19911733,  0.00208542, -0.19174016, -0.16148713,  0.15155935,\n",
       "         -0.22503065,  0.14657217,  0.2858503 ,  0.19155729,  0.0814797 ,\n",
       "          0.04162309, -0.26380247,  0.01191908, -0.2045396 , -0.02893409,\n",
       "         -0.00524753, -0.03329998,  0.24538672, -0.20503488, -0.12765431,\n",
       "         -0.00398839,  0.03764468, -0.04951662,  0.166823  , -0.2673344 ,\n",
       "         -0.10174929,  0.04103369,  0.29724246, -0.05886355,  0.16813993,\n",
       "          0.24093312,  0.13788405,  0.10813567, -0.125619  , -0.2547833 ,\n",
       "         -0.11664677, -0.20493707,  0.2701218 ,  0.15458292, -0.2469199 ,\n",
       "         -0.04731834,  0.14895204,  0.1300379 , -0.28921616, -0.2609642 ,\n",
       "         -0.11647254, -0.03390771,  0.03300896, -0.05949734,  0.28055346,\n",
       "         -0.18021148, -0.18531382,  0.05403385, -0.12303579,  0.19652948,\n",
       "         -0.00815246,  0.01398316, -0.29414326,  0.24666566,  0.27371287,\n",
       "         -0.28226635,  0.09346783, -0.16788478,  0.21942025]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_6/bias:0' shape=(64,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_7/kernel:0' shape=(64, 10) dtype=float32, numpy=\n",
       " array([[-6.45234734e-02,  2.80007392e-01, -1.84060901e-01,\n",
       "         -6.27102852e-02, -1.29084826e-01,  4.30100262e-02,\n",
       "          2.81001300e-01, -5.51976860e-02,  1.76297307e-01,\n",
       "          1.96535736e-01],\n",
       "        [-2.11855024e-01, -1.67935163e-01,  1.78977102e-01,\n",
       "          6.05988801e-02, -8.78202170e-02, -1.89500660e-01,\n",
       "          8.33690166e-03, -1.18678987e-01, -2.47733384e-01,\n",
       "          4.46226597e-02],\n",
       "        [-6.56435639e-02,  1.78271323e-01, -1.74976587e-02,\n",
       "          1.41074061e-02, -1.10241726e-01, -2.00968549e-01,\n",
       "          1.71320230e-01,  2.23332047e-02,  1.36261582e-02,\n",
       "          2.39998668e-01],\n",
       "        [ 1.30838126e-01,  1.44881576e-01,  2.47905940e-01,\n",
       "         -2.46105686e-01,  1.41936213e-01,  1.73854411e-01,\n",
       "          2.48857230e-01,  2.83701450e-01,  1.79517925e-01,\n",
       "          1.98919326e-01],\n",
       "        [ 9.62917507e-02,  6.09260798e-02, -2.09848851e-01,\n",
       "          1.63815022e-01,  4.45510447e-02,  1.06423855e-01,\n",
       "         -1.23683155e-01,  1.48145348e-01,  1.64207637e-01,\n",
       "         -1.48013234e-03],\n",
       "        [-1.51029959e-01,  8.53406191e-02, -2.64347941e-01,\n",
       "         -2.16131434e-01, -3.84117067e-02,  9.72908139e-02,\n",
       "          1.02480710e-01, -3.42143178e-02,  6.24603331e-02,\n",
       "         -2.36800134e-01],\n",
       "        [-7.77793527e-02,  2.45131224e-01, -2.13089585e-02,\n",
       "         -4.13547009e-02, -1.94433063e-01, -2.21891209e-01,\n",
       "          9.39351916e-02,  2.00988173e-01,  2.37917095e-01,\n",
       "          5.99555373e-02],\n",
       "        [ 3.43700647e-02,  2.65236706e-01,  2.60610729e-01,\n",
       "         -1.22267008e-03,  2.46965885e-03,  1.61793172e-01,\n",
       "          2.72644669e-01,  2.23315656e-02, -2.42642254e-01,\n",
       "         -9.81714576e-02],\n",
       "        [ 2.29760408e-02, -1.18459970e-01,  2.82601714e-02,\n",
       "         -1.75863981e-01, -1.76709414e-01, -2.38256961e-01,\n",
       "         -2.38371626e-01,  1.70393080e-01, -1.44449472e-02,\n",
       "         -1.04801059e-02],\n",
       "        [ 1.72897965e-01,  1.42855227e-01,  2.94146240e-02,\n",
       "         -2.57523686e-01, -3.32756937e-02, -3.19016278e-02,\n",
       "         -2.28980601e-01, -1.67878538e-01,  1.69325531e-01,\n",
       "          1.09034449e-01],\n",
       "        [ 7.75145888e-02, -7.20139444e-02, -1.57134682e-01,\n",
       "          3.30656469e-02, -5.92171848e-02,  2.24562854e-01,\n",
       "         -2.60657638e-01,  5.75043559e-02,  2.70368189e-01,\n",
       "         -6.10153675e-02],\n",
       "        [-1.30109742e-01, -1.06434643e-01, -8.95498246e-02,\n",
       "         -2.61146903e-01, -2.22271532e-01, -1.88822180e-01,\n",
       "         -5.90093136e-02,  4.42120731e-02,  1.54864609e-01,\n",
       "          1.88445807e-01],\n",
       "        [ 2.53533125e-02,  1.21789247e-01, -2.06719369e-01,\n",
       "         -9.38153267e-03,  2.22601563e-01, -1.30265683e-01,\n",
       "         -2.04062864e-01, -1.37534440e-02,  5.50014973e-02,\n",
       "         -8.13880563e-02],\n",
       "        [ 1.25349820e-01,  4.66740131e-02,  5.62014878e-02,\n",
       "         -1.58956140e-01,  2.07063764e-01, -2.82596052e-01,\n",
       "         -1.37177467e-01,  1.81643456e-01, -5.13504744e-02,\n",
       "          1.41564876e-01],\n",
       "        [ 7.90635347e-02, -1.32505894e-02, -1.46996260e-01,\n",
       "          7.34975338e-02, -7.24635720e-02,  2.75042504e-01,\n",
       "          1.31116867e-01, -8.92284364e-02,  1.34904325e-01,\n",
       "         -3.37148607e-02],\n",
       "        [ 6.90728426e-02, -2.10316122e-01, -1.32038534e-01,\n",
       "         -2.60953903e-02,  1.74421132e-01, -2.07939059e-01,\n",
       "         -2.24379137e-01, -2.37472981e-01,  1.15827233e-01,\n",
       "         -1.82775289e-01],\n",
       "        [ 2.58782595e-01, -1.87975526e-01,  2.01063186e-01,\n",
       "          3.33671272e-02,  2.69584924e-01, -1.56586617e-01,\n",
       "          2.73273796e-01,  2.40986437e-01, -8.40592980e-02,\n",
       "          1.09855860e-01],\n",
       "        [ 2.28277177e-01, -1.38345659e-02,  3.44439149e-02,\n",
       "         -1.58809096e-01, -1.34932369e-01,  1.28303826e-01,\n",
       "         -2.05067903e-01,  1.00412697e-01, -5.86010367e-02,\n",
       "          1.44167662e-01],\n",
       "        [ 2.77616113e-01,  1.29475653e-01,  1.99171036e-01,\n",
       "         -2.73076028e-01,  1.41253531e-01,  2.34827310e-01,\n",
       "          1.98040009e-02,  1.14486843e-01, -2.68725634e-01,\n",
       "         -1.50827512e-01],\n",
       "        [ 1.22608393e-01,  1.50010258e-01, -5.59264421e-03,\n",
       "          1.97669059e-01, -8.35647881e-02, -1.23710454e-01,\n",
       "          1.51217252e-01,  6.37557209e-02,  1.45149618e-01,\n",
       "         -1.47008896e-01],\n",
       "        [-1.18192762e-01, -2.26218700e-02,  5.15342653e-02,\n",
       "         -1.76954761e-01, -2.43088305e-02,  2.03249544e-01,\n",
       "         -2.55703509e-01,  5.84721565e-03,  1.49698913e-01,\n",
       "         -2.35195369e-01],\n",
       "        [ 2.08371311e-01,  1.94682628e-01, -1.23994768e-01,\n",
       "          1.65275246e-01,  1.47488594e-01, -2.24593878e-01,\n",
       "         -2.65247196e-01, -1.09789595e-01,  1.65024132e-01,\n",
       "          1.90944672e-01],\n",
       "        [-3.73276472e-02,  1.62197113e-01,  2.38022536e-01,\n",
       "          6.97028041e-02,  1.37495458e-01,  2.28577346e-01,\n",
       "         -2.52532333e-01,  4.77191806e-03,  1.59726828e-01,\n",
       "         -2.02017501e-01],\n",
       "        [-1.39349639e-01, -1.65602833e-01,  5.33806980e-02,\n",
       "          1.94156140e-01,  2.54286379e-01, -2.56437302e-01,\n",
       "         -1.26560494e-01, -2.74260938e-02,  1.63649917e-02,\n",
       "         -1.05485961e-01],\n",
       "        [-2.09996581e-01,  2.71645218e-01, -2.76721537e-01,\n",
       "         -2.19472185e-01,  1.92375749e-01, -8.88325125e-02,\n",
       "         -2.83942431e-01,  4.30748761e-02,  1.28548622e-01,\n",
       "         -2.07023576e-01],\n",
       "        [-2.20832229e-02, -1.62901655e-01,  5.35158813e-02,\n",
       "         -1.45153612e-01,  1.86764300e-02, -2.46667743e-01,\n",
       "         -2.01712877e-01, -2.79143006e-01, -1.51529282e-01,\n",
       "         -1.96896151e-01],\n",
       "        [ 1.28512859e-01, -2.15915889e-01,  2.08440751e-01,\n",
       "          2.82134265e-01,  1.84251070e-02, -8.46357346e-02,\n",
       "          1.49926543e-03, -2.80290276e-01,  6.17599785e-02,\n",
       "          5.15952110e-02],\n",
       "        [ 3.65827680e-02, -1.37519017e-01, -2.75076330e-02,\n",
       "         -2.43092418e-01,  1.24571294e-01, -2.40648761e-01,\n",
       "         -2.64877200e-01,  1.08927339e-01,  2.56898016e-01,\n",
       "          1.43611521e-01],\n",
       "        [-2.45429844e-01, -2.38860354e-01, -1.60679370e-01,\n",
       "          2.29763836e-01, -1.70073718e-01, -2.65632629e-01,\n",
       "          1.12583160e-01,  6.04228377e-02,  2.58971125e-01,\n",
       "          2.27433950e-01],\n",
       "        [-2.31774569e-01, -1.69552863e-02, -1.73508972e-01,\n",
       "          1.61041021e-01,  1.35323882e-01, -6.81000054e-02,\n",
       "         -1.41506478e-01, -6.77450746e-02,  1.70100003e-01,\n",
       "         -2.56137311e-01],\n",
       "        [-2.68140286e-01, -9.82295126e-02,  1.09148622e-02,\n",
       "         -2.16001764e-01,  2.84675092e-01,  7.19768703e-02,\n",
       "          2.24200785e-02,  7.75173008e-02, -9.80809629e-02,\n",
       "         -1.84102863e-01],\n",
       "        [-1.93253964e-01, -1.09143212e-01, -1.50502056e-01,\n",
       "         -2.63597310e-01, -1.81785613e-01,  1.13813251e-01,\n",
       "          1.24161631e-01, -2.43998736e-01, -1.70088738e-01,\n",
       "          8.40776861e-02],\n",
       "        [ 3.48224044e-02,  5.46803772e-02, -2.03498304e-02,\n",
       "         -1.51985094e-01, -1.39785022e-01,  1.60486430e-01,\n",
       "          1.62320316e-01,  2.64501840e-01, -2.48579293e-01,\n",
       "         -1.31005540e-01],\n",
       "        [-2.68789053e-01, -1.14131033e-01,  2.59781212e-01,\n",
       "         -1.54009268e-01,  7.73632526e-02,  1.93624914e-01,\n",
       "          2.42333442e-01, -9.34723914e-02, -2.15995908e-02,\n",
       "         -1.58343911e-02],\n",
       "        [-2.17857182e-01, -1.40983596e-01,  9.03480649e-02,\n",
       "         -2.02880576e-01,  6.86285198e-02, -2.40460768e-01,\n",
       "          2.42737383e-01,  2.60477871e-01,  1.32515430e-02,\n",
       "          1.63024724e-01],\n",
       "        [-2.60376096e-01,  6.30914867e-02,  2.06779391e-01,\n",
       "         -2.39621997e-01,  2.42916673e-01,  1.54988378e-01,\n",
       "          8.22196305e-02, -5.83674908e-02, -6.61115944e-02,\n",
       "         -2.01722175e-01],\n",
       "        [ 2.67013818e-01, -2.60616630e-01, -1.14645839e-01,\n",
       "          9.65499878e-02, -1.24612972e-01,  3.78006995e-02,\n",
       "          1.82423711e-01,  1.96732879e-02, -7.78070539e-02,\n",
       "          2.57673949e-01],\n",
       "        [-2.01018512e-01, -1.38558671e-01, -2.77061522e-01,\n",
       "         -2.03569114e-01, -2.42927119e-01, -2.61473238e-01,\n",
       "          7.01537132e-02,  6.58473074e-02,  6.08925521e-02,\n",
       "         -2.74254978e-02],\n",
       "        [ 2.51359195e-01,  2.31843203e-01, -2.05950379e-01,\n",
       "         -2.52753496e-04,  2.16434568e-01, -1.99928418e-01,\n",
       "          3.58275175e-02,  1.63478374e-01,  2.79611677e-01,\n",
       "          1.90646023e-01],\n",
       "        [ 9.37563777e-02,  2.50335187e-01, -2.00499356e-01,\n",
       "         -1.88131332e-02, -5.77337444e-02, -2.05402374e-01,\n",
       "          1.47876710e-01, -4.59575653e-03, -4.75450903e-02,\n",
       "          1.26986295e-01],\n",
       "        [-1.84410200e-01,  1.26178950e-01, -6.69388324e-02,\n",
       "         -2.02932179e-01,  1.60068721e-01,  2.04004467e-01,\n",
       "          1.35026067e-01, -2.47978464e-01, -5.65038621e-02,\n",
       "         -8.97773802e-02],\n",
       "        [-1.57171339e-01, -1.42347425e-01,  1.91343367e-01,\n",
       "         -2.07897574e-01, -2.31575966e-03,  3.71949375e-02,\n",
       "         -1.41601056e-01, -4.95859683e-02,  2.48566478e-01,\n",
       "         -1.39413461e-01],\n",
       "        [-1.06235728e-01,  1.99956864e-01,  1.70191854e-01,\n",
       "          2.31932610e-01, -9.32202488e-02,  5.71360588e-02,\n",
       "          3.26749384e-02, -5.50680757e-02,  1.84267372e-01,\n",
       "         -1.00306436e-01],\n",
       "        [-2.48538211e-01, -2.32229218e-01, -2.28301838e-01,\n",
       "         -2.02256441e-02, -5.96052408e-03, -3.65368724e-02,\n",
       "         -1.26286224e-01, -1.32073089e-01,  2.48609096e-01,\n",
       "          3.74836624e-02],\n",
       "        [ 1.81652695e-01,  1.51528209e-01, -2.48921454e-01,\n",
       "          1.31553322e-01, -1.27422899e-01, -2.43119717e-01,\n",
       "         -2.33642817e-01,  2.12046474e-01,  1.99708045e-01,\n",
       "         -1.65466696e-01],\n",
       "        [ 2.34606475e-01,  2.76121765e-01, -2.02223212e-01,\n",
       "          2.00091094e-01, -2.63850987e-01, -2.39171222e-01,\n",
       "          1.07386321e-01,  2.16249019e-01, -2.51919776e-01,\n",
       "         -1.80485934e-01],\n",
       "        [ 2.51010150e-01,  9.49564278e-02, -4.18801606e-02,\n",
       "          2.36824542e-01,  1.30622357e-01,  4.91622686e-02,\n",
       "         -3.67190987e-02, -2.54031658e-01,  2.63214141e-01,\n",
       "          2.23619729e-01],\n",
       "        [-2.72622883e-01,  1.47659481e-01, -1.67002350e-01,\n",
       "         -1.74746186e-01, -1.09339356e-02,  7.56982863e-02,\n",
       "          2.64855057e-01,  6.98127151e-02, -2.17461646e-01,\n",
       "         -7.06696063e-02],\n",
       "        [-2.23252520e-01,  2.35717028e-01, -3.66618633e-02,\n",
       "         -8.65676552e-02, -1.78527951e-01, -1.61463767e-01,\n",
       "         -1.46768630e-01, -6.50010705e-02,  7.64629245e-02,\n",
       "          1.06239945e-01],\n",
       "        [ 4.24689651e-02, -4.41682935e-02, -2.21187145e-01,\n",
       "          2.34689504e-01,  1.32554621e-01,  5.00174761e-02,\n",
       "         -1.24374062e-01, -4.46321070e-02, -3.21862698e-02,\n",
       "          5.53464293e-02],\n",
       "        [-1.12729669e-02,  2.06547678e-02,  1.43998682e-01,\n",
       "         -1.65293038e-01,  5.02994955e-02,  2.50071377e-01,\n",
       "          2.58296132e-02, -2.20760658e-01,  2.51774818e-01,\n",
       "          4.05144989e-02],\n",
       "        [ 7.59125948e-02, -6.20154440e-02,  1.19501948e-01,\n",
       "         -3.95318866e-03,  1.00768149e-01,  1.27335578e-01,\n",
       "         -8.59197974e-03,  1.31337851e-01, -2.61045158e-01,\n",
       "          1.31651491e-01],\n",
       "        [ 9.94313359e-02,  1.25794023e-01,  1.59606725e-01,\n",
       "          1.18701726e-01, -1.60401568e-01,  1.72968447e-01,\n",
       "         -1.79982543e-01,  1.93758190e-01, -1.79972231e-01,\n",
       "          2.33124405e-01],\n",
       "        [ 1.32159650e-01, -1.25910461e-01, -1.72899544e-01,\n",
       "          1.71724170e-01, -4.83784080e-03, -1.96250796e-01,\n",
       "          1.80206358e-01, -8.40210021e-02,  1.20445520e-01,\n",
       "          1.84729487e-01],\n",
       "        [-2.70419598e-01,  3.70755196e-02, -2.18791872e-01,\n",
       "         -2.02006638e-01, -8.41778219e-02,  1.06143951e-01,\n",
       "          2.36726671e-01, -1.79336160e-01, -8.80521536e-03,\n",
       "         -2.02568620e-01],\n",
       "        [-2.41267294e-01,  1.99752301e-01, -1.22299582e-01,\n",
       "          9.43977833e-02,  5.28086722e-02,  1.71380311e-01,\n",
       "         -1.02399588e-01, -1.23356342e-01, -2.09050775e-02,\n",
       "         -6.26921654e-03],\n",
       "        [-6.70228004e-02, -1.59974545e-01, -2.06790656e-01,\n",
       "         -2.50600010e-01, -4.13888395e-02, -2.52856314e-01,\n",
       "         -2.63152301e-01, -1.98427945e-01, -1.35678068e-01,\n",
       "         -9.37038660e-03],\n",
       "        [ 7.50612915e-02,  8.09488893e-02, -1.93391994e-01,\n",
       "          8.51149559e-02,  2.38704681e-03,  2.32845098e-01,\n",
       "          1.28939062e-01, -2.52013266e-01, -6.27371669e-02,\n",
       "         -2.64444828e-01],\n",
       "        [-5.40212989e-02, -3.90625596e-02, -2.69203722e-01,\n",
       "         -5.82373440e-02, -1.23212814e-01, -1.50253043e-01,\n",
       "          1.18307352e-01, -1.80544674e-01,  1.59664303e-01,\n",
       "          2.66188413e-01],\n",
       "        [-9.92215723e-02, -1.61985084e-01, -8.59529972e-02,\n",
       "         -1.49570554e-01,  1.75851822e-01, -1.18940428e-01,\n",
       "          1.17800623e-01, -9.22941715e-02, -2.39275426e-01,\n",
       "         -2.45301604e-01],\n",
       "        [-1.04616314e-01,  1.55427128e-01,  1.39836282e-01,\n",
       "         -7.78026432e-02,  6.53485954e-02,  2.49896944e-02,\n",
       "         -3.54405344e-02,  8.58952105e-02, -1.50375098e-01,\n",
       "          2.46362716e-01],\n",
       "        [ 3.68239880e-02, -4.73412871e-02, -1.02857307e-01,\n",
       "          2.17565686e-01, -3.96796018e-02,  1.49216294e-01,\n",
       "         -1.41110718e-02,  4.96769547e-02, -2.83999741e-02,\n",
       "         -4.27777916e-02],\n",
       "        [ 3.38992476e-02, -2.25681871e-01,  1.67044729e-01,\n",
       "          1.80475146e-01, -6.43633157e-02,  1.03942156e-01,\n",
       "         -2.08557457e-01,  1.45889729e-01,  2.79259473e-01,\n",
       "          1.58271223e-01],\n",
       "        [-1.08275712e-02, -1.44886196e-01, -6.94752932e-02,\n",
       "          2.22769976e-02,  2.12323070e-01, -5.36618233e-02,\n",
       "          7.54532814e-02,  2.35710412e-01,  2.40893751e-01,\n",
       "          2.74540782e-02]], dtype=float32)>,\n",
       " <tf.Variable 'dense_7/bias:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.build(input_shape=(None, 3))\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**The summary method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 64)                256       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 906\n",
      "Trainable params: 906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()#我們會通過model.summary()輸出模型各層的參數狀況 ＊＊＊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Naming models and layers with the `name` argument**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_example_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " my_first_layer (Dense)      (None, 64)                256       \n",
      "                                                                 \n",
      " my_last_layer (Dense)       (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 906\n",
      "Trainable params: 906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(name=\"my_example_model\")\n",
    "model.add(layers.Dense(64, activation=\"relu\", name=\"my_first_layer\"))\n",
    "model.add(layers.Dense(10, activation=\"softmax\", name=\"my_last_layer\"))\n",
    "model.build((None, 3))#建立模型\n",
    "model.summary()\n",
    "#(3+1)*64 = 256\n",
    "#(64+1)*10 =650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Specifying the input shape of your model in advance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.add(Dense(7, input_shape=(4,)))  # Dense就是常用的全連接層\n",
    "model.add(Activation('sigmoid')) # 激活函數\n",
    "\n",
    "隱層\n",
    "model.add(Dense(13))  # Dense就是常用的全連接層\n",
    "model.add(Activation('sigmoid')) # 激活函數\n",
    "\n",
    "輸出層\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "第一個Dense層，輸入數據維度是4（一維數據），有7個神經元。\n",
    "所以，Param=(4+1)*7=35.\n",
    "第二個Dense層，輸入數據維度是7（經過第一層7個神經元作用後，輸出數據維度就是7了），有13個神經元。\n",
    "所以，Param=(7+1)*13=104.\n",
    "第三個Dense層，輸入數據維度是13（經過第二層13個神經元作用後，輸出數據維度就是13了），有5個神經元。\n",
    "所以，Param=(13+1)*5=70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(2,)))\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "# (2+1)*64 =192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_14 (Dense)            (None, 64)                192       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 192\n",
      "Trainable params: 192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 64)                192       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 842\n",
      "Trainable params: 842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(layers.Dense(10, activation=\"softmax\"))\n",
    "model.summary()\n",
    "#補充：Param = （輸入數據維度+1）* 神經元個數，之所以要加1，是考慮到每個神經元都有一個Bias。\n",
    "# *** 上一層的神經元個數=這一層的數據維度\n",
    "#-> dense_12 : (2+1)*64\n",
    "#-> dense_13:(64+1)*10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### The Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### A simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**A simple Functional model with two `Dense` layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "與Sequential模型不同，您必須創建獨立的Input層物件的instance並定義輸入數據張量的維度形狀(tensor shape)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#在每個隱藏層中使用relu激活函數，並且在輸出層中使用softmax激活函數進行多類別分類。\n",
    "inputs = keras.Input(shape=(3,), name=\"my_input\")#該模型有3個輸入\n",
    "features = layers.Dense(64, activation=\"relu\")(inputs)#一個隱藏層 64個隱藏神經元\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(features)#輸出層有10個輸出\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#我們要把MNIST的每張圖像(28x28)打平成一個一維(784)的張量做為一個多層感知器(MLP)的Input\n",
    "#ex:mnist_input=Input(shape=(784,))\n",
    "inputs = keras.Input(shape=(3,), name=\"my_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "features = layers.Dense(64, activation=\"relu\")(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " my_input (InputLayer)       [(None, 3)]               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                256       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 906\n",
      "Trainable params: 906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()#印出網絡結構\n",
    "#補充：Param = （輸入數據維度+1）* 神經元個數，之所以要加1，是考慮到每個神經元都有一個Bias。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Multi-input, multi-output models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**A multi-input, multi-output Functional model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#多輸入多輸出模型\n",
    "vocabulary_size = 10000 # Size of vocabulary obtained when preprocessing text data\n",
    "num_tags = 100 # Number of unique issue tags\n",
    "num_departments = 4 # Number of departments for predictions\n",
    "\n",
    "\n",
    "title = keras.Input(shape=(vocabulary_size,), name=\"title\")\n",
    "text_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\n",
    "tags = keras.Input(shape=(num_tags,), name=\"tags\")\n",
    "\n",
    "features = layers.Concatenate()([title, text_body, tags])#layers.Concatenate()將兩個層結合\n",
    "features = layers.Dense(64, activation=\"relu\")(features)#隱藏層\n",
    "\n",
    "priority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)#輸出層1\n",
    "department = layers.Dense(\n",
    "    num_departments, activation=\"softmax\", name=\"department\")(features)#輸出層2\n",
    "\n",
    "model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.functional.Functional object at 0x7fde9ee151e0>\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " title (InputLayer)             [(None, 10000)]      0           []                               \n",
      "                                                                                                  \n",
      " text_body (InputLayer)         [(None, 10000)]      0           []                               \n",
      "                                                                                                  \n",
      " tags (InputLayer)              [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 20100)        0           ['title[0][0]',                  \n",
      "                                                                  'text_body[0][0]',              \n",
      "                                                                  'tags[0][0]']                   \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 64)           1286464     ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " priority (Dense)               (None, 1)            65          ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " department (Dense)             (None, 4)            260         ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,286,789\n",
      "Trainable params: 1,286,789\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Training a multi-input, multi-output model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training a model by providing lists of input & target arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 6ms/step - loss: 33.9061 - priority_loss: 0.3258 - department_loss: 33.5803 - priority_mean_absolute_error: 0.4894 - department_accuracy: 0.2352\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 21.6423 - priority_loss: 0.3285 - department_loss: 21.3138 - priority_mean_absolute_error: 0.4923 - department_accuracy: 0.0633\n",
      "40/40 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "#訓練\n",
    "#選項 1：提供輸入和目標數組的列表\n",
    "\n",
    "#首先，我們創建一些值介於 0 和 2 之間的隨機輸入數據以及用於標籤的隨機數據。\n",
    "#我們將使用這些數據來訓練我們的模型：\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "num_samples = 1280#定義樣本數\n",
    "\n",
    "#創造隨機的輸入數據 Create random input data\n",
    "title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
    "tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\n",
    "\n",
    "#創造用於標籤的隨機數據 Create random labels\n",
    "priority_data = np.random.random(size=(num_samples, 1))\n",
    "department_data = np.random.randint(0, 2, size=(num_samples, num_departments))\n",
    "\n",
    "# Compile model # 編譯: 選擇損失函數、優化方法及成效衡量方式\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n",
    "              metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\n",
    "#進行訓練\n",
    "model.fit([title_data, text_body_data, tags_data],\n",
    "          [priority_data, department_data],\n",
    "          epochs=1)\n",
    "#評估(Evaluation)：訓練完後，計算成效。\n",
    "model.evaluate([title_data, text_body_data, tags_data],\n",
    "               [priority_data, department_data])\n",
    "priority_preds, department_preds = model.predict([title_data, text_body_data, tags_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training a model by providing dicts of input & target arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 6ms/step - loss: 40.9657 - priority_loss: 0.3285 - department_loss: 40.6372 - priority_mean_absolute_error: 0.4923 - department_accuracy: 0.2500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 36.0209 - priority_loss: 0.3285 - department_loss: 35.6924 - priority_mean_absolute_error: 0.4923 - department_accuracy: 0.5758\n",
      "40/40 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Use title names.\n",
    "#输入是按顺序传入的，我们也可以通过字典的方式传入\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss={\"priority\": \"mean_squared_error\", \"department\": \"categorical_crossentropy\"},\n",
    "              metrics={\"priority\": [\"mean_absolute_error\"], \"department\": [\"accuracy\"]})\n",
    "model.fit({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
    "          {\"priority\": priority_data, \"department\": department_data},\n",
    "          epochs=1)\n",
    "model.evaluate({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
    "               {\"priority\": priority_data, \"department\": department_data})\n",
    "priority_preds, department_preds = model.predict(\n",
    "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### The power of the Functional API: Access to layer connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, \"ticket_classifier.png\")\n",
    "#這邊會印出一張分枝圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, \"ticket_classifier_with_shape_info.png\", show_shapes=True)\n",
    "#這邊也會印出一張分支圖"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Retrieving the inputs or outputs of a layer in a Functional model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x7fb25d177fd0>,\n",
       " <keras.engine.input_layer.InputLayer at 0x7fb25d1901f0>,\n",
       " <keras.engine.input_layer.InputLayer at 0x7fb25d191cc0>,\n",
       " <keras.layers.merging.concatenate.Concatenate at 0x7fb25d174cd0>,\n",
       " <keras.layers.core.dense.Dense at 0x7fb25d1927a0>,\n",
       " <keras.layers.core.dense.Dense at 0x7fb25d192530>,\n",
       " <keras.layers.core.dense.Dense at 0x7fb25d192bf0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 10000) dtype=float32 (created by layer 'title')>,\n",
       " <KerasTensor: shape=(None, 10000) dtype=float32 (created by layer 'text_body')>,\n",
       " <KerasTensor: shape=(None, 100) dtype=float32 (created by layer 'tags')>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[3].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 20100) dtype=float32 (created by layer 'concatenate')>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[3].output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Creating a new model by reusing intermediate layer outputs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "features = model.layers[4].output\n",
    "difficulty = layers.Dense(3, activation=\"softmax\", name=\"difficulty\")(features)\n",
    "\n",
    "new_model = keras.Model(\n",
    "    inputs=[title, text_body, tags],\n",
    "    outputs=[priority, department, difficulty])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "#plot\n",
    "keras.utils.plot_model(new_model, \"updated_ticket_classifier.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Subclassing the Model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Rewriting our previous example as a subclassed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**A simple subclassed model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#Subclassing the Model Class集成模型子類\n",
    "class CustomerTicketModel(keras.Model): \n",
    "\n",
    "    def __init__(self, num_departments):\n",
    "        super().__init__()\n",
    "        self.concat_layer = layers.Concatenate()\n",
    "        self.mixing_layer = layers.Dense(64, activation=\"relu\")\n",
    "        self.priority_scorer = layers.Dense(1, activation=\"sigmoid\")\n",
    "        self.department_classifier = layers.Dense(\n",
    "            num_departments, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        title = inputs[\"title\"]\n",
    "        text_body = inputs[\"text_body\"]\n",
    "        tags = inputs[\"tags\"]\n",
    "\n",
    "        features = self.concat_layer([title, text_body, tags])\n",
    "        features = self.mixing_layer(features)\n",
    "        priority = self.priority_scorer(features)\n",
    "        department = self.department_classifier(features)\n",
    "        return priority, department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "#初始化 model\n",
    "model = CustomerTicketModel(num_departments=4)\n",
    "\n",
    "priority, department = model(\n",
    "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 1s 6ms/step - loss: 30.9617 - output_1_loss: 0.3237 - output_2_loss: 30.6379 - output_1_mean_absolute_error: 0.4880 - output_2_accuracy: 0.2414\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 24.1405 - output_1_loss: 0.3285 - output_2_loss: 23.8119 - output_1_mean_absolute_error: 0.4923 - output_2_accuracy: 0.1242\n",
      "40/40 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "#訓練model\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n",
    "              metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\n",
    "model.fit({\"title\": title_data,\n",
    "           \"text_body\": text_body_data,\n",
    "           \"tags\": tags_data},\n",
    "          [priority_data, department_data],\n",
    "          epochs=1)\n",
    "model.evaluate({\"title\": title_data,\n",
    "                \"text_body\": text_body_data,\n",
    "                \"tags\": tags_data},\n",
    "               [priority_data, department_data])\n",
    "priority_preds, department_preds = model.predict({\"title\": title_data,\n",
    "                                                  \"text_body\": text_body_data,\n",
    "                                                  \"tags\": tags_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Beware: What subclassed models don't support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Mixing and matching different components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Creating a Functional model that includes a subclassed model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class Classifier(keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        if num_classes == 2:\n",
    "            num_units = 1\n",
    "            activation = \"sigmoid\"\n",
    "        else:\n",
    "            num_units = num_classes\n",
    "            activation = \"softmax\"\n",
    "        self.dense = layers.Dense(num_units, activation=activation)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "inputs = keras.Input(shape=(3,))\n",
    "features = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "outputs = Classifier(num_classes=10)(features)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Creating a subclassed model that includes a Functional model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(64,))\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(inputs)\n",
    "binary_classifier = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "class MyModel(keras.Model):\n",
    "\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.dense = layers.Dense(64, activation=\"relu\")\n",
    "        self.classifier = binary_classifier\n",
    "\n",
    "    def call(self, inputs):\n",
    "        features = self.dense(inputs)\n",
    "        return self.classifier(features)\n",
    "\n",
    "model = MyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Remember: Use the right tool for the job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Using built-in training and evaluation loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**The standard workflow: `compile()`, `fit()`, `evaluate()`, `predict()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2942 - accuracy: 0.9116 - val_loss: 0.1438 - val_accuracy: 0.9584\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1662 - accuracy: 0.9531 - val_loss: 0.1165 - val_accuracy: 0.9682\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1396 - accuracy: 0.9623 - val_loss: 0.1121 - val_accuracy: 0.9706\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9711\n",
      "313/313 [==============================] - 0s 999us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "def get_mnist_model():\n",
    "    inputs = keras.Input(shape=(28 * 28,))\n",
    "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "    features = layers.Dropout(0.5)(features)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
    "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
    "train_images, val_images = images[10000:], images[:10000]\n",
    "train_labels, val_labels = labels[10000:], labels[:10000]\n",
    "\n",
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=3,\n",
    "          validation_data=(val_images, val_labels))\n",
    "test_metrics = model.evaluate(test_images, test_labels)\n",
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Writing your own metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Implementing a custom metric by subclassing the `Metric` class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class RootMeanSquaredError(keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name=\"rmse\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.mse_sum = self.add_weight(name=\"mse_sum\", initializer=\"zeros\")\n",
    "        self.total_samples = self.add_weight(\n",
    "            name=\"total_samples\", initializer=\"zeros\", dtype=\"int32\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.one_hot(y_true, depth=tf.shape(y_pred)[1])\n",
    "        mse = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "        self.mse_sum.assign_add(mse)\n",
    "        num_samples = tf.shape(y_pred)[0]\n",
    "        self.total_samples.assign_add(num_samples)\n",
    "\n",
    "    def result(self):\n",
    "        return tf.sqrt(self.mse_sum / tf.cast(self.total_samples, tf.float32))\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.mse_sum.assign(0.)\n",
    "        self.total_samples.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.2924 - accuracy: 0.9129 - rmse: 7.1808 - val_loss: 0.1513 - val_accuracy: 0.9559 - val_rmse: 7.3665\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1660 - accuracy: 0.9534 - rmse: 7.3527 - val_loss: 0.1233 - val_accuracy: 0.9667 - val_rmse: 7.4053\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1389 - accuracy: 0.9627 - rmse: 7.3865 - val_loss: 0.1156 - val_accuracy: 0.9710 - val_rmse: 7.4227\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9710 - rmse: 7.4345\n"
     ]
    }
   ],
   "source": [
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\", RootMeanSquaredError()])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=3,\n",
    "          validation_data=(val_images, val_labels))\n",
    "test_metrics = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Using callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### The EarlyStopping and ModelCheckpoint callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Using the `callbacks` argument in the `fit()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.2936 - accuracy: 0.9136 - val_loss: 0.1621 - val_accuracy: 0.9537\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1645 - accuracy: 0.9543 - val_loss: 0.1323 - val_accuracy: 0.9653\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1404 - accuracy: 0.9627 - val_loss: 0.1177 - val_accuracy: 0.9708\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1261 - accuracy: 0.9683 - val_loss: 0.1065 - val_accuracy: 0.9741\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1154 - accuracy: 0.9712 - val_loss: 0.1119 - val_accuracy: 0.9744\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1103 - accuracy: 0.9737 - val_loss: 0.1017 - val_accuracy: 0.9758\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1092 - accuracy: 0.9744 - val_loss: 0.1048 - val_accuracy: 0.9766\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.0979 - accuracy: 0.9768 - val_loss: 0.1129 - val_accuracy: 0.9770\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.0978 - accuracy: 0.9772 - val_loss: 0.1149 - val_accuracy: 0.9782\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.0930 - accuracy: 0.9788 - val_loss: 0.1187 - val_accuracy: 0.9774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb25d1b04f0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=2,\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"checkpoint_path.keras\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "    )\n",
    "]\n",
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"checkpoint_path.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Writing your own callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Creating a custom callback by subclassing the `Callback` class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs):\n",
    "        self.per_batch_losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.per_batch_losses.append(logs.get(\"loss\"))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        plt.clf()\n",
    "        plt.plot(range(len(self.per_batch_losses)), self.per_batch_losses,\n",
    "                 label=\"Training loss for each batch\")\n",
    "        plt.xlabel(f\"Batch (epoch {epoch})\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"plot_at_epoch_{epoch}\")\n",
    "        self.per_batch_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 0.2969 - accuracy: 0.9116 - val_loss: 0.1455 - val_accuracy: 0.9582\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1669 - accuracy: 0.9534 - val_loss: 0.1206 - val_accuracy: 0.9673\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1399 - accuracy: 0.9631 - val_loss: 0.1119 - val_accuracy: 0.9709\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1266 - accuracy: 0.9664 - val_loss: 0.1099 - val_accuracy: 0.9727\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1194 - accuracy: 0.9703 - val_loss: 0.1097 - val_accuracy: 0.9744\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1086 - accuracy: 0.9728 - val_loss: 0.1087 - val_accuracy: 0.9760\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1059 - accuracy: 0.9742 - val_loss: 0.1061 - val_accuracy: 0.9768\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.1031 - accuracy: 0.9763 - val_loss: 0.1019 - val_accuracy: 0.9781\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.0968 - accuracy: 0.9781 - val_loss: 0.1134 - val_accuracy: 0.9783\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 0.0962 - accuracy: 0.9786 - val_loss: 0.1084 - val_accuracy: 0.9784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb25ec1afb0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          callbacks=[LossHistory()],\n",
    "          validation_data=(val_images, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Monitoring and visualization with TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-24 09:44:09.833021: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at summary_kernels.cc:65 : PERMISSION_DENIED: /full_path_to_your_log_dir; Read-only file system\n"
     ]
    },
    {
     "ename": "PermissionDeniedError",
     "evalue": "/full_path_to_your_log_dir; Read-only file system [Op:CreateSummaryFileWriter]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      6\u001b[0m tensorboard \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(\n\u001b[1;32m      7\u001b[0m     log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/full_path_to_your_log_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n\u001b[0;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7164\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7163\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7164\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: /full_path_to_your_log_dir; Read-only file system [Op:CreateSummaryFileWriter]"
     ]
    }
   ],
   "source": [
    "model = get_mnist_model()\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "tensorboard = keras.callbacks.TensorBoard(\n",
    "    log_dir=\"/full_path_to_your_log_dir\",\n",
    ")\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          validation_data=(val_images, val_labels),\n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-4b3350509f9e09a4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-4b3350509f9e09a4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /full_path_to_your_log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Writing your own training and evaluation loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Training versus inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Low-level usage of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: 1.00\n"
     ]
    }
   ],
   "source": [
    "metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "targets = [0, 1, 2]\n",
    "predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "metric.update_state(targets, predictions)\n",
    "current_result = metric.result()\n",
    "print(f\"result: {current_result:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of values: 2.00\n"
     ]
    }
   ],
   "source": [
    "values = [0, 1, 2, 3, 4]\n",
    "mean_tracker = keras.metrics.Mean()\n",
    "for value in values:\n",
    "    mean_tracker.update_state(value)\n",
    "print(f\"Mean of values: {mean_tracker.result():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### A complete training and evaluation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Writing a step-by-step training loop: the training step function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "model = get_mnist_model()\n",
    "\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = keras.optimizers.RMSprop()\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "loss_tracking_metric = keras.metrics.Mean()\n",
    "\n",
    "def train_step(inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(targets, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[metric.name] = metric.result()\n",
    "\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"loss\"] = loss_tracking_metric.result()\n",
    "    return logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Writing a step-by-step training loop: resetting the metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "def reset_metrics():\n",
    "    for metric in metrics:\n",
    "        metric.reset_state()\n",
    "    loss_tracking_metric.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Writing a step-by-step training loop: the loop itself**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results at the end of epoch 0\n",
      "...sparse_categorical_accuracy: 0.9147\n",
      "...loss: 0.2908\n",
      "Results at the end of epoch 1\n",
      "...sparse_categorical_accuracy: 0.9538\n",
      "...loss: 0.1670\n",
      "Results at the end of epoch 2\n",
      "...sparse_categorical_accuracy: 0.9620\n",
      "...loss: 0.1420\n"
     ]
    }
   ],
   "source": [
    "training_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "training_dataset = training_dataset.batch(32)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    reset_metrics()\n",
    "    for inputs_batch, targets_batch in training_dataset:\n",
    "        logs = train_step(inputs_batch, targets_batch)\n",
    "    print(f\"Results at the end of epoch {epoch}\")\n",
    "    for key, value in logs.items():\n",
    "        print(f\"...{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Writing a step-by-step evaluation loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "...val_sparse_categorical_accuracy: 0.9672\n",
      "...val_loss: 0.1355\n"
     ]
    }
   ],
   "source": [
    "def test_step(inputs, targets):\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss = loss_fn(targets, predictions)\n",
    "\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[\"val_\" + metric.name] = metric.result()\n",
    "\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"val_loss\"] = loss_tracking_metric.result()\n",
    "    return logs\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "reset_metrics()\n",
    "for inputs_batch, targets_batch in val_dataset:\n",
    "    logs = test_step(inputs_batch, targets_batch)\n",
    "print(\"Evaluation results:\")\n",
    "for key, value in logs.items():\n",
    "    print(f\"...{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Make it fast with tf.function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Adding a `tf.function` decorator to our evaluation-step function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "...val_sparse_categorical_accuracy: 0.9672\n",
      "...val_loss: 0.1355\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def test_step(inputs, targets):\n",
    "    predictions = model(inputs, training=False)\n",
    "    loss = loss_fn(targets, predictions)\n",
    "\n",
    "    logs = {}\n",
    "    for metric in metrics:\n",
    "        metric.update_state(targets, predictions)\n",
    "        logs[\"val_\" + metric.name] = metric.result()\n",
    "\n",
    "    loss_tracking_metric.update_state(loss)\n",
    "    logs[\"val_loss\"] = loss_tracking_metric.result()\n",
    "    return logs\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = val_dataset.batch(32)\n",
    "reset_metrics()\n",
    "for inputs_batch, targets_batch in val_dataset:\n",
    "    logs = test_step(inputs_batch, targets_batch)\n",
    "print(\"Evaluation results:\")\n",
    "for key, value in logs.items():\n",
    "    print(f\"...{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Leveraging fit() with a custom training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Implementing a custom training step to use with `fit()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
    "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        inputs, targets = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(inputs, training=True)\n",
    "            loss = loss_fn(targets, predictions)\n",
    "        gradients = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
    "\n",
    "        loss_tracker.update_state(loss)\n",
    "        return {\"loss\": loss_tracker.result()}\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [loss_tracker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.2918\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.1639\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.1381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb25f97a140>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(28 * 28,))\n",
    "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "features = layers.Dropout(0.5)(features)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model = CustomModel(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop())\n",
    "model.fit(train_images, train_labels, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def train_step(self, data):\n",
    "        inputs, targets = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(inputs, training=True)\n",
    "            loss = self.compiled_loss(targets, predictions)\n",
    "        gradients = tape.gradient(loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
    "        self.compiled_metrics.update_state(targets, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.2947 - sparse_categorical_accuracy: 0.9119\n",
      "Epoch 2/3\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.1648 - sparse_categorical_accuracy: 0.9531\n",
      "Epoch 3/3\n",
      "1563/1563 [==============================] - 4s 2ms/step - loss: 0.1384 - sparse_categorical_accuracy: 0.9623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb25f9b86a0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(28 * 28,))\n",
    "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
    "features = layers.Dropout(0.5)(features)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
    "model = CustomModel(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "model.fit(train_images, train_labels, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter07_working-with-keras.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
